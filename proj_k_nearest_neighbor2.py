from __future__ import division
import os
os.chdir(r'D:\Haverford\2017-2018\Chem 362')

import pickle, time
from Python_Exercise.linear_algebra import distance
from collections import Counter

def majority_vote(labels):
    """assumes that labels are ordered from nearest to farthest"""
    vote_counts = Counter(labels)
    winner, winner_count = vote_counts.most_common(1)[0]
    num_winners = len([count 
                       for count in vote_counts.values()
                       if count == winner_count])

    if num_winners == 1:
        return winner                     # unique winner, so return it
    else:
        return majority_vote(labels[:-1]) # try again without the farthest
    
def knn_classify(k, labeled_points, new_point):
    # each labeled point should be a pair (point, label)
    # order the labeled points from nearest to farthest
    by_distance = sorted(labeled_points,
                         key=lambda args: distance(args[0], new_point))

    # find the labels for the k closest
    k_nearest_labels = [label for _, label in by_distance[:k]]

    # and let them vote
    return majority_vote(k_nearest_labels)

with open('cleandata.p', 'rb') as f:
    rawdata, reality, Zscore_exempted_cols, coldict, rowdict = pickle.load(f)

krange = [1, 2, 3, 5] # the number of nearest neighbors

traintestratio = []
correctness = [[] for _ in krange]
timeused = [[] for _ in krange]

bmin = 1000
bmax = 2000
bgap = 50
domaingap = 1
mintraingrpsz = 300
b = 0
testset = set(range(2500, len(rawdata))) # Remains constant over time

print(len(testset))

# For each training/testing set, 
while b in range(0, bmax - bmin, bgap):
    trainset = set(range(0, b + bmin, domaingap))
    validationrows = set()
    if trainset.intersection(testset) != set():
        print('Choose a training/testing set that does not coincide with one another')
        pass
    
    train_rawdata = [rawdata[index] for index in trainset]
    test_rawdata = [rawdata[index] for index in testset]
    train_reality = [reality[index] for index in trainset]
    test_reality = [reality[index] for index in testset]
    
    traindata = list(zip(train_rawdata, train_reality))
    testdata = list(zip(test_rawdata, test_reality))
    
    comparison = []
    traintestratio.append(len(traindata)/len(testdata))
    
    print('When training and testing sets divided at', b + bmin)
    
    # if __name__ == "__main__":
    # try several different values for k
    for numk, k in enumerate(krange):
        num_correct = 0
        kstart = time.clock()

        for dataset, result in testdata:
            predicted_result = knn_classify(k, traindata, dataset)            
            comparison.append([predicted_result, result]) # Make a list of all the outcomes
            
            if predicted_result == result:
                num_correct += 1

        print(k, "neighbor[s]:", num_correct, "correct out of", len(testdata))
        correctness[numk].append(num_correct/len(testdata))
        print(comparison)
        
        kend = time.clock()
        timeused[numk].append(kend - kstart)
        
    b += bgap
    # Change size of training and test set and make learning curve out of it
    
    # TASK
    # INCORPORATE RESULTS GENERATED BY RANDOM CHOICE
    # INCORPORATE MATHEMATICAL EXPECTATIONS

plotuse = correctness, traintestratio, timeused, krange

with open('plotdata.p', 'wb') as g:
    g.seek(0)
    g.truncate() # Erase everything before moving forward
    pickle.dump(plotuse,g)